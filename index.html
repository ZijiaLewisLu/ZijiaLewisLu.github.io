<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Zijia Lu - PhD Candidate in Computer Science specializing in Video Understanding Methods">
    <title>Zijia Lu | Computer Science Researcher</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <!-- Left Sidebar -->
        <div class="sidebar">
            <div class="profile">
                <!-- <img src="./images/photo.jpg" alt="Zijia Lu" class="profile-image"> -->
                <img src="./images/cartoon.png" alt="Zijia Lu" class="profile-image">
                <div class="profile-info">
                    <h1 class="profile-name">Zijia Lu</h1>
                    <p class="profile-title">Ph.D. Candidate in Computer Science</p>
                </div>
            </div>
            
            <div class="contact-info">
                <div class="contact-item">
                    <img src="symbols/email.png" alt="Email Icon" class="contact-icon">
                    <a href="mailto:lu.zij@northeastern.edu">lu.zij@northeastern.edu</a>
                </div>
                <div class="contact-item">
                    <img src="symbols/icons8-google-scholar-50.png" alt="Google Scholar Icon" class="contact-icon">
                    <a href="https://scholar.google.com/citations?user=xEGL7NsAAAAJ&hl=en&oi=ao" target="_blank" rel="noopener">Google Scholar</a>
                </div>
                <div class="contact-item">
                    <img src="symbols/icons8-github-30.png" alt="Github Icon" class="contact-icon">
                    <a href="https://github.com/ZijiaLewisLu" target="_blank" rel="noopener">Github</a>
                </div>
                <div class="contact-item">
                    <img src="symbols/icons8-linkedin-circled-50.png" alt="LinkedIn Icon" class="contact-icon">
                    <a href="https://www.linkedin.com/in/zijialewislu" target="_blank" rel="noopener">LinkedIn</a>
                </div>
            </div>
            
            <nav class="sidebar-nav">
                <ul>
                    <li><a href="#about" class="active">About Me</a></li>
                    <li><a href="#updates">Recent Updates</a></li>
                    <li><a href="#publications">Publications</a></li>
                    <li><a href="#experience">Experience</a></li>
                </ul>
            </nav>
        </div>
        
        <!-- Main Content -->
        <div class="main-content">
            <!-- About Section -->
            <div class="section" id="about">
                <h2 class="section-title">About Me</h2>
                <div class="about-content">
                    <p>
                    I am a final-year Ph.D. Candidate in Computer Science at <org>Northeastern University</org>. 
                    My research focuses on <strong>Video Understanding Methods</strong> to address key challenges in <strong>Video-Language Models (VLM) and Assistant AI</strong>. 
                    Specifically, it includes data-efficient video-text matching, enhanced computational efficiency, accurate long temporal modeling, and procedural video understanding. 
                    Currently, I am working on <i>open-world video understanding</i> and <i>video data generation</i>.
                    </p>
                    <p>During my Ph.D. study, I have had the opportunity to spend two memorable internships at <org>Amazon AWS AI Lab</org> and <org>Microsoft Research</org>.
                    In the past, I also worked as a Student Researcher at <org>Chinese Academic of Sciences</org>, and an Architecture Summer Intern at <org>NVIDIA</org>.
                    In 2019, I received dual B.S. degrees in Computer Science and Economics from <org>NYU Shanghai</org>, and awarded with the NYU University Honors Scholar and the Undergraduate Scholarship of University of Chinese Academy of Sciences.
                    </p>
                    <p><i>I am actively seeking full-time Research Scientist or Applied Scientist positions starting in 2025. Please reach out to me if you have an opportunity!</i></p>
                </div>
            </div>
            
            <!-- Updates Section -->
            <div class="section" id="updates">
                <h2 class="section-title">Recent Updates</h2>
                <ul class="updates-list">
                    <li class="update-item">
                        <span class="update-date">Apr 2025</span>
                        <span class="update-content">My intern paper at Microsoft is accepted to CVPR 2025.</span>
                    </li>
                    <li class="update-item">
                        <span class="update-date">Jun 2024</span>
                        <span class="update-content">Started Research Internship at Microsoft Responsible and Open AI Research Team.</span>
                    </li>
                    <li class="update-item">
                        <span class="update-date">Apr 2024</span>
                        <span class="update-content">Three papers accepted to CVPR 2024. My intern paper at Amazon is designated as "Highlight" paper.</span>
                    </li>
                    <li class="update-item">
                        <span class="update-date">Sep 2022</span>
                        <span class="update-content">Started as an Applied Scientist Intern at Amazon's AWS AI Lab.</span>
                    </li>
                    <li class="update-item">
                        <span class="update-date">Apr 2022</span>
                        <span class="update-content">One paper accepted to CVPR 2022.</span>
                    </li>
                </ul>
            </div>
            
            <!-- Combined Publications Section -->
            <div class="section" id="publications">
                <h2 class="section-title">Selected Publications</h2>
                <div class="research-container">

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/decaf2.png" alt="DeCafNet">
                                </div>
                                <h3 class="publication-title">DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, A S M Iftekhar, Gaurav Mittal, Tianjian Meng, Xiawei Wang, Cheng Zhao, Rohith Kukkala, Ehsan Elhamifar, Mei Chen</p>
                                <p class="publication-venue">CVPR 2025</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <!-- <ul class="publication-description" style="margin:0; padding-left:5px;"> -->
                                <ul class="publication-description">
                                    <li class="regular">A Delegate-and-Conquer framework for efficient coarse-to-fine <span>Long Video Temporal Grounding</span>. </li>
                                    <li class="regular"><span>Efficient Video Encoder</span> for end-to-end training on Ego4D with one A100. </li>
                                    <li class="regular">SOTA accuracy with 47%-66% lower computation.</li>
                                    <li class="star">APPLICATION: Improve <span>VLLMs</span> to efficiently handle hour-long videos.</li>
                                    <!-- <li style="list-style-type: none; position: relative; padding-left: -5px;"> <span class="publication-t2">Application</span>: Improve VLLM's memory load and processing speed for long videos.</li> -->
                                </ul>
                                <!-- <span class="publication-tag">Grounding</span> Test -->
                                <!-- <p class="publication-description"><org>APPLICATION</org>: Improve VLLM's memory load and processing speed for long videos.</p> -->
                            </div>
                        </div>
                        <!-- <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="https://cvpr.thecvf.com/virtual/2025/poster/32761">Paper</a>
                                <a>Code (TBA)</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Temporal Video Grounding</span>
                                <span class="publication-tag">Efficient Video Encoder</span>
                            </div>
                        </div> -->
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/fact.png" alt="FACT">
                                </div>
                                <h3 class="publication-title">FACT: Frame-Action Cross-Attention Temporal Modeling for Efficient Fully-Supervised Action Segmentation</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Ehsan Elhamifar</p>
                                <p class="publication-venue">CVPR 2024</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <p class="publication-description">
                                    Proposed an efficient framework of synchronized temporal modeling on multi-levels (frame/action). Achieved state-of-the-art results on 4 datasets with lower inference time and enabled Vision-Language Learning. <br>
                                    APPLICATION: New efficient temporal network; Multi-Modal Learning; Reduce Data Requirement.
                                </p>
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_FACT_Frame-Action_Cross-Attention_Temporal_Modeling_for_Efficient_Action_Segmentation_CVPR_2024_paper.pdf">Paper</a>
                                <a href="https://github.com/ZijiaLewisLu/CVPR2024-FACT">Code</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Long Temporal Modeling</span>
                                <span class="publication-tag">Multi-Modal Learning</span>
                                <span class="publication-tag">Model Efficiency</span>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/pcl.png" alt="Self-Supervised Multi-Object Tracking">
                                </div>
                                <h3 class="publication-title">Self-Supervised Multi-Object Tracking with Path Consistency</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Bing Shuai, Yanbei Chen, Zhenlin Xu, Davide Modolo</p>
                                <p class="publication-venue">CVPR 2024 Highlight</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <p class="publication-description">
                                    Proposed a new tracking self-supervision objective with improved robustness to occlusion and appearance changes. Outperformed existing works on popular benchmarks. <br>
                                    APPLICATION: Robust object tracking for social interaction or egocentric video understanding.

                                </p>
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_Self-Supervised_Multi-Object_Tracking_with_Path_Consistency_CVPR_2024_paper.pdf">Paper</a>
                                <a href="https://github.com/amazon-science/path-consistency">Code</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Multi-Object Tracking</span>
                                <span class="publication-tag">Self-Supervised Learning</span>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/egoper.png" alt="Error Detection">
                                </div>
                                <h3 class="publication-title">Error Detection in Egocentric Procedural Task Videos</h3>
                                <p class="publication-authors">Shih-Po Lee, <strong>Zijia Lu</strong>, Zekun Zhang, Minh Hoai, Ehsan Elhamifar</p>
                                <p class="publication-venue">CVPR 2024</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <p class="publication-description">
                                    Proposed Spatio-Temporal & Contrastive Learning method for error detection in procedural videos. Collected Egocentric Procedural Error Detection dataset of extensive error types. <br>
                                    APPLICATION: New dataset for procedural video understanding, error detection and Assistant AI.
                                </p>
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_Error_Detection_in_Egocentric_Procedural_Task_Videos_CVPR_2024_paper.pdf">Paper</a>
                                <a href="https://github.com/robert80203/EgoPER_official">Code & Dataset</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Egocentric Understanding</span>
                                <span class="publication-tag">Error Detection</span>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/poc.png" alt="Set-Supervised Action Learning">
                                </div>
                                <h3 class="publication-title">Set-Supervised Action Learning in Procedural Task Videos via Pairwise Order Consistency</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Ehsan Elhamifar</p>
                                <p class="publication-venue">CVPR 2022</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <p class="publication-description">
                                    Proposed enhanced Action Modeling via Multi-Dimension Subspaces to capture large intra-class variations in weakly supervised video-text learning scenarios. <br>
                                    APPLICATION: learn shared video-text semantic space without human labels. Support better video generation and understanding.
                                </p>
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="http://www.khoury.northeastern.edu/home/eelhami/publications/setSupSegmentation-CVPR22.pdf">Paper</a>
                                <a href="https://github.com/ZijiaLewisLu/CVPR22-POC">Code</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Video-Text Alignment</span>
                                <span class="publication-tag">Differentiable Sequence Metric</span>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/tasl.png" alt="Weakly-supervised Action Segmentation">
                                </div>
                                <h3 class="publication-title">Weakly-supervised Action Segmentation and Alignment via Transcript-Aware Union-of-subspaces Learning</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Ehsan Elhamifar</p>
                                <p class="publication-venue">ICCV 2022</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <p class="publication-description">
                                    Proposed Self-Supervised method to Recover and Learn Action Temporal Dependencies; Doubled the accuracies of state-of-the-art approaches for weakly supervised video-text learning.<br>
                                    APPLICATION: New Action Modeling Approach. learn shared video-text semantic space without human label.
                                </p>
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="http://www.khoury.northeastern.edu/home/eelhami/publications/TASL-ICCV21.pdf">Paper</a>
                                <a href="https://github.com/ZijiaLewisLu/ICCV21-TASL">Code</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Video-Text Alignment</span>
                                <span class="publication-tag">Weakly-supervised Learning</span>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/dft-net.png" alt="Zero-Shot Facial Expression Recognition">
                                </div>
                                <h3 class="publication-title">Dft-Net: Disentanglement of Face Deformation and Texture Synthesis for Expression Editing</h3>
                                <p class="publication-authors">Jinghui Wang, Jie Zhang, <strong>Zijia Lu</strong>, Shiguang Shan</p>
                                <p class="publication-venue">ICIP 2019</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <!-- <p class="publication-description">Proposed a new Transductive Label Propagation method and the first Open-Set Facial Expression Recognition dataset.</p> -->
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="https://ieeexplore.ieee.org/abstract/document/8803416">Paper</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Image Generation</span>
                                <span class="publication-tag">Facial Editing</span>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/zml2p.png" alt="Zero-Shot Facial Expression Recognition">
                                </div>
                                <h3 class="publication-title">Zero-Shot Facial Expression Recognition with Multi-Label Label Propagation</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Jiabei Zeng, Shiguang Shan, Xilin Chen</p>
                                <p class="publication-venue">ACCV 2018 Oral</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <p class="publication-description">
                                    Proposed a new Transductive Label Propagation method and the first Open-Set Facial Expression Recognition dataset.<br>
                                    APPLICATION: Open-World Facial Expression. First Complex Facial Expression Recognition Dataset.
                                </p>
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="https://link.springer.com/chapter/10.1007/978-3-030-20893-6_2">Paper</a>
                                <a href="https://github.com/ZijiaLewisLu/ACCV18-ZML2P">Code & Dataset</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Affection Recognition</span>
                                <span class="publication-tag">Zero-Shot Learning</span>
                            </div>
                        </div>
                    </div>
                    
                </div>
            </div>

            <!-- Experience Section -->
            <div class="section" id="experience">
                <h2 class="section-title">Experience</h2>
                <div class="experience-container">
                    <div class="experience-item">
                        <img src="./symbols/Microsoft.jpg" alt="Microsoft" class="experience-image">
                        <div class="experience-info">
                            <div class="experience-meta">
                                <h3 class="experience-title">Research Intern at Microsoft</h3>
                                <!-- <span class="experience-organization">Microsoft Responsible and Open AI Research Team</span> -->
                                <span class="experience-duration">06/2024 - 09/2024</span>
                            </div>
                            <p class="experience-description">Worked on efficient end-to-end models for text-based video temporal grounding.</p>
                        </div>
                    </div>
                    <div class="experience-item">
                        <img src="./symbols/amazon-logo-on-transparent-background-free-vector.jpg"" alt="Amazon" class="experience-image">
                        <div class="experience-info">
                            <div class="experience-meta">
                                <h3 class="experience-title">Applied Scientist Intern at Amazon AWS AI Lab</h3>
                                <!-- <span class="experience-organization">Amazon AWS AI Lab</span> -->
                                <span class="experience-duration">09/2022 - 03/2023</span>
                            </div>
                            <p class="experience-description">Worked on Self-Supervised and Robust Multi-Object Tracking.</p>
                        </div>
                    </div>
                    <div class="experience-item">
                        <img src="./symbols/northeastern.png" alt="Northeastern University" class="experience-image">
                        <div class="experience-info">
                            <!-- <span class="experience-duration">2019 - Present</span> -->
                            <div class="experience-meta">
                                <!-- <h3 class="experience-title">Graduate Research Assistant in Northeastern University</h3> -->
                                <h3 class="experience-title">Graduate Research Assistant in Northeastern University</h3>
                                <!-- <span class="experience-organization">Northeastern University</span> -->
                                <span class="experience-duration">09/2019 - Present</span>
                            </div>
                            <p class="experience-description">Focusing on Video-Text Understanding, Action Segmentation, Egocentric Understanding and Video Data Generation.</p>
                        </div>
                    </div>
                    <div class="experience-item">
                        <img src="./symbols/cas2.jpg" alt="" class="experience-image">
                        <div class="experience-info">
                            <div class="experience-meta">
                                <h3 class="experience-title">Student Researcher at Chinese Academy of Science</h3>
                                <!-- <span class="experience-organization">NYU Shanghai</span> -->
                                <span class="experience-duration">01/2018 - 12/2018</span>
                            </div>
                            <p class="experience-description">Worked on Zero-Shot Facial Recognition and Expression Editing.</p>
                        </div>
                    </div>
                    <div class="experience-item">
                        <img src="symbols/nvidia2.png" alt="NVIDIA" class="experience-image">
                        <div class="experience-info">
                            <div class="experience-meta">
                                <h3 class="experience-title">Architecture Summer Intern at NVIDIA Shanghai</h3>
                                <!-- <span class="experience-organization">NYU Shanghai</span> -->
                                <span class="experience-duration">06/2016 - 09/2016</span>
                            </div>
                            <p class="experience-description">Developed CUDNN function for Volta GPU and GPU performance simulator.</p>
                        </div>
                    </div>
                    <div class="experience-item">
                        <img src="symbols/nyush2.jpg" alt="NYU Shanghai" class="experience-image">
                        <div class="experience-info">
                            <div class="experience-meta">
                                <h3 class="experience-title">B.S in Computer Science and Economics at NYU Shanghai</h3>
                                <!-- <span class="experience-organization">NYU Shanghai</span> -->
                                <span class="experience-duration">2014 - 2019 (Gap Year 2018)</span>
                            </div>
                            <p class="experience-description">Graduated with a Major GPA of 3.98/4. Conducted research in Image Segmentation with Prof. Zhen Zhang and Document QA with Prof. Kyunghyun Cho.</p>
                        </div>
                    </div>
                    
                </div>
            </div>
        </div>
    </div>
    
    <!-- Footer -->
    <footer>
        <p>&copy; 2025 Zijia Lu. Last updated: April 2025</p>
    </footer>
    
    <script src="script.js"></script>
</body>
</html>