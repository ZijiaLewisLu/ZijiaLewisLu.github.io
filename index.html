<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>About Me - Zijia Lu</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <!-- Left Sidebar -->
        <div class="sidebar">
            <div class="profile">
                <img src="./images/photo.jpg" alt="Zijia Lu" class="profile-image">
                <div class="profile-info">
                    <h1 class="profile-name">Zijia Lu</h1>
                    <p class="profile-title">Ph.D. Candidate in Computer Science</p>
                </div>
            </div>
            
            <div class="contact-info">
                <!-- <h3>Contact</h3> -->
                <div class="contact-item">
                    <!-- <span class="contact-icon">üìß</span> -->
                    <img src="symbols/email.png" alt="Email Icon" class="contact-icon">
                    <span>lu.zij at northeastern dot edu</span>
                </div>
                <!-- <div class="contact-item">
                    <span class="contact-icon">üìû</span>
                    <span>857-654-0908</span>
                </div> -->
                <!-- <div class="contact-item">
                    <span class="contact-icon">üè¢</span>
                    <span>Northeastern University</span>
                </div> -->
                <!-- <div class="contact-item">
                    <span class="contact-icon">üìç</span>
                    <span>Boston, USA</span>
                </div> -->
            <!-- </div> -->
            
            <!-- <div class="contact-info"> -->
                <!-- <h3>Links</h3> -->
                <!-- <div class="contact-item">
                    <span class="contact-icon">üìÑ</span>
                    <a href="#">CV</a>
                </div> -->
                <div class="contact-item">
                    <!-- <span class="contact-icon">üîç</span> -->
                    <img src="symbols/icons8-google-scholar-50.png" alt="Github Icon" class="contact-icon">
                    <a href="https://scholar.google.com/citations?user=xEGL7NsAAAAJ&hl=en&oi=ao">Google Scholar</a>
                </div>
                <div class="contact-item">
                    <!-- <span class="contact-icon">üîç</span> -->
                    <img src="symbols/icons8-github-30.png" alt="Github Icon" class="contact-icon">
                    <a href="https://github.com/ZijiaLewisLu">Github</a>
                </div>
                <div class="contact-item">
                    <!-- <span class="contact-icon">üîç</span> -->
                    <img src="symbols/icons8-linkedin-circled-50.png" alt="LinkedIn Icon" class="contact-icon">
                    <a href="https://www.linkedin.com/in/zijialewislu">LinkedIn</a>
                </div>
                <!-- <div class="social-links">
                    <a href="#" title="GitHub">GitHub</a>
                    <a href="#" title="LinkedIn">LinkedIn</a>
                </div> -->
            </div>
            <nav class="sidebar-nav">
                <ul>
                    <li><a href="#about">About Me</a></li>
                    <li><a href="#updates">Updates</a></li>
                    <li><a href="#experience">Experience</a></li>
                    <li><a href="#publications">Publications</a></li>
                    <li><a href="#honors">Honors & Awards</a></li>
                </ul>
            </nav>
        </div>
        
        <!-- Main Content -->
        <div class="main-content">
            <!-- About Section -->
            <div class="section" id="about">
                <h2 class="section-title">About Me</h2>
                <div class="about-content">
                    <p>I am a Ph.D. Candidate in Computer Science at Northeastern University, focusing on Video-Text Understanding. My research interests include open-world video understanding, temporal modeling, action segmentation, and weakly supervised video-text learning.</p>
                    <p>Prior to my Ph.D., I received dual B.S. degrees in Computer Science & Economics from NYU Shanghai with a Major GPA of 3.98/4. I have extensive research and industry experience, having worked as a Research Assistant with Prof. Ehsan Elhamifar, a Research Intern at Microsoft's Responsible and Open AI Research Team, an Applied Scientist Intern at Amazon's AWS AI Lab, a Student Researcher at Chinese Academic of Sciences, and an Architecture Team Summer Intern at NVIDIA.</p>
                    <p>My research aims to advance the understanding of video content through efficient temporal modeling, action segmentation, error detection, and multi-object tracking. I am actively looking for collaboration opportunities in computer vision and video understanding.</p>
                </div>
            </div>
            
            <!-- Updates Section -->
            <div class="section" id="updates">
                <h2 class="section-title">Updates</h2>
                <ul class="updates-list">
                    <li class="update-item">
                        <span class="update-date">04-2025</span>
                        <span class="update-content">My intern paper at Microsoft is accepted to CVPR 2025.</span>
                    </li>
                    <li class="update-item">
                        <span class="update-date">06-2024</span>
                        <span class="update-content">Started Research Internship at Microsoft Responsible and Open AI Research Team.</span>
                    </li>
                    <li class="update-item">
                        <span class="update-date">04-2024</span>
                        <span class="update-content">Three papers accepted to CVPR 2024. My intern paper as Amazon is designated as "Highlight" paper.</span>
                    </li>
                    <li class="update-item">
                        <span class="update-date">09-2022</span>
                        <span class="update-content">Started as an Applied Scientist Intern at Amazon's AWS AI Lab.</span>
                    </li>
                    <li class="update-item">
                        <span class="update-date">04-2022</span>
                        <span class="update-content">One papers accepted to CVPR 2022.</span>
                    </li>
                    <!-- <li class="update-item">
                        <span class="update-date">06-2021</span>
                        <span class="update-content">One papers accepted to ICCV 2021.</span>
                    </li> -->
                    <!-- <li class="update-item">
                        <span class="update-date">09-2019</span>
                        <span class="update-content">Began Ph.D. studies at Northeastern University with Prof. Ehsan Elhamifar.</span>
                    </li> -->
                </ul>
            </div>

            <!-- Experience Section -->
            <div class="section" id="experience">
                <h2 class="section-title">Experience</h2>
                <div class="experience-container">
                    <div class="experience-item">
                        <img src="./symbols/Microsoft.jpg" alt="Microsoft" class="experience-image">
                        <div class="experience-info">
                            <div class="experience-meta">
                                <h3 class="experience-title">Research Intern at Microsoft</h3>
                                <!-- <span class="experience-organization">Microsoft Responsible and Open AI Research Team</span> -->
                                <span class="experience-duration">06/2024 - 09/2024</span>
                            </div>
                            <p class="experience-description">Worked on advancing responsible AI technologies with a focus on video understanding.</p>
                        </div>
                    </div>
                    <div class="experience-item">
                        <img src="./symbols/amazon-logo-on-transparent-background-free-vector.jpg"" alt="Amazon" class="experience-image">
                        <div class="experience-info">
                            <div class="experience-meta">
                                <h3 class="experience-title">Applied Scientist Intern at Amazon AWS AI Lab</h3>
                                <!-- <span class="experience-organization">Amazon AWS AI Lab</span> -->
                                <span class="experience-duration">09/2022 - 03/2023</span>
                            </div>
                            <!-- <p class="experience-description">Developed state-of-the-art models for video understanding and action segmentation.</p> -->
                        </div>
                    </div>
                    <div class="experience-item">
                        <img src="./symbols/northeastern.png" alt="Northeastern University" class="experience-image">
                        <div class="experience-info">
                            <!-- <span class="experience-duration">2019 - Present</span> -->
                            <div class="experience-meta">
                                <!-- <h3 class="experience-title">Graduate Research Assistant in Northeastern University</h3> -->
                                <h3 class="experience-title">Graduate Research Assistant in Northeastern University</h3>
                                <!-- <span class="experience-organization">Northeastern University</span> -->
                                <span class="experience-duration">09/2019 - Present</span>
                            </div>
                            <!-- <p class="experience-description">Focusing on Video-Text Understanding, including open-world video understanding, temporal modeling, and weakly supervised video-text learning.</p> -->
                        </div>
                    </div>
                    <div class="experience-item">
                        <img src="./symbols/cas2.jpg" alt="" class="experience-image">
                        <div class="experience-info">
                            <div class="experience-meta">
                                <h3 class="experience-title">Student Researcher at Chinese Academy of Science</h3>
                                <!-- <span class="experience-organization">NYU Shanghai</span> -->
                                <span class="experience-duration">01/2018/01 - 12/2018</span>
                            </div>
                            <!-- <p class="experience-description">Graduated with a Major GPA of 3.98/4. Conducted research in computer vision and machine learning.</p> -->
                        </div>
                    </div>
                    <!-- <div class="experience-item">
                        <img src="./images/nyu.png" alt="NYU Shanghai" class="experience-image">
                        <div class="experience-info">
                            <h3 class="experience-title">Dual B.S. Degrees in Computer Science & Economics</h3>
                            <div class="experience-meta">
                                <span class="experience-organization">NYU Shanghai</span>
                                <span class="experience-duration">2015 - 2019</span>
                            </div>
                            <p class="experience-description">Graduated with a Major GPA of 3.98/4. Conducted research in computer vision and machine learning.</p>
                        </div>
                    </div> -->
                </div>
            </div>
            
            <!-- Combined Publications Section -->
            <div class="section" id="publications">
                <h2 class="section-title">Selected Publications</h2>
                <div class="research-container">

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/decaf2.png" alt="DeCafNet">
                                </div>
                                <h3 class="publication-title">DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, A S M Iftekhar, Gaurav Mittal, Tianjian Meng, Xiawei Wang, Cheng Zhao, Rohith Kukkala, Ehsan Elhamifar, Mei Chen</p>
                                <p class="publication-venue">CVPR 2025</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <p class="publication-description">Proposed a novel Delegate-and-Conquer method that achieves state-of-the-art performance with 47%-66% lower computation for efficient temporal sentence grounding in hour-long videos.</p>
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="https://cvpr.thecvf.com/virtual/2025/poster/32761">Paper</a>
                                <a href="#">Code</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Temporal Video Grounding</span>
                                <span class="publication-tag">Efficient Video Encoder</span>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/fact.png" alt="FACT">
                                </div>
                                <h3 class="publication-title">FACT: Frame-Action Cross-Attention Temporal Modeling for Efficient Fully-Supervised Action Segmentation</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Ehsan Elhamifar</p>
                                <p class="publication-venue">CVPR 2024</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <p class="publication-description">Proposed an efficient framework of synchronized temporal modeling on multi-levels (frame/action). Achieved state-of-the-art results on 4 datasets with lower inference time and enabled Vision-Language Learning.</p>
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_FACT_Frame-Action_Cross-Attention_Temporal_Modeling_for_Efficient_Action_Segmentation_CVPR_2024_paper.pdf">Paper</a>
                                <a href="https://github.com/ZijiaLewisLu/CVPR2024-FACT">Code</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Long Temporal Modeling</span>
                                <span class="publication-tag">Multi-Modal Learning</span>
                                <span class="publication-tag">Model Efficiency</span>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/pcl.png" alt="Self-Supervised Multi-Object Tracking">
                                </div>
                                <h3 class="publication-title">Self-Supervised Multi-Object Tracking with Path Consistency</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Bing Shuai, Yanbei Chen, Zhenlin Xu, Davide Modolo</p>
                                <p class="publication-venue">CVPR 2024 Highlight</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <p class="publication-description">Proposed a new tracking self-supervision objective with improved robustness to occlusion and appearance changes. Outperformed existing works on popular benchmarks.</p>
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_Self-Supervised_Multi-Object_Tracking_with_Path_Consistency_CVPR_2024_paper.pdf">Paper</a>
                                <a href="https://github.com/amazon-science/path-consistency">Code</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Multi-Object Tracking</span>
                                <span class="publication-tag">Self-Supervised Learning</span>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/egoper.png" alt="Error Detection">
                                </div>
                                <h3 class="publication-title">Error Detection in Egocentric Procedural Task Videos</h3>
                                <p class="publication-authors">Shih-Po Lee, <strong>Zijia Lu</strong>, Zekun Zhang, Minh Hoai, Ehsan Elhamifar</p>
                                <p class="publication-venue">CVPR 2024</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <p class="publication-description">Proposed Spatio-Temporal & Contrastive Learning method for error detection in procedural videos. Collected Egocentric Procedural Error Detection dataset of extensive error types.</p>
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_Error_Detection_in_Egocentric_Procedural_Task_Videos_CVPR_2024_paper.pdf">Paper</a>
                                <a href="https://github.com/robert80203/EgoPER_official">Code & Dataset</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Egocentric Understanding</span>
                                <span class="publication-tag">Error Detection</span>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/poc.png" alt="Set-Supervised Action Learning">
                                </div>
                                <h3 class="publication-title">Set-Supervised Action Learning in Procedural Task Videos via Pairwise Order Consistency</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Ehsan Elhamifar</p>
                                <p class="publication-venue">CVPR 2022</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <p class="publication-description">Proposed enhanced Action Modeling via Multi-Dimension Subspaces to capture large intra-class variations in weakly supervised video-text learning scenarios.</p>
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="http://www.khoury.northeastern.edu/home/eelhami/publications/setSupSegmentation-CVPR22.pdf">Paper</a>
                                <a href="https://github.com/ZijiaLewisLu/CVPR22-POC">Code</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Video-Text Alignment</span>
                                <span class="publication-tag">Differentiable Sequence Metric</span>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/tasl.png" alt="Weakly-supervised Action Segmentation">
                                </div>
                                <h3 class="publication-title">Weakly-supervised Action Segmentation and Alignment via Transcript-Aware Union-of-subspaces Learning</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Ehsan Elhamifar</p>
                                <p class="publication-venue">ICCV 2022</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <p class="publication-description">Proposed Self-Supervised method to Recover and Learn Action Temporal Dependencies; Doubled the accuracies of state-of-the-art approaches for weakly supervised video-text learning.</p>
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="http://www.khoury.northeastern.edu/home/eelhami/publications/TASL-ICCV21.pdf">Paper</a>
                                <a href="https://github.com/ZijiaLewisLu/ICCV21-TASL">Code</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Video-Text Alignment</span>
                                <span class="publication-tag">Weakly-supervised Learning</span>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/dft-net.png" alt="Zero-Shot Facial Expression Recognition">
                                </div>
                                <h3 class="publication-title">Dft-Net: Disentanglement of Face Deformation and Texture Synthesis for Expression Editing</h3>
                                <p class="publication-authors">Jinghui Wang, Jie Zhang, <strong>Zijia Lu</strong>, Shiguang Shan</p>
                                <p class="publication-venue">ICIP 2019</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <!-- <p class="publication-description">Proposed a new Transductive Label Propagation method and the first Open-Set Facial Expression Recognition dataset.</p> -->
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="https://ieeexplore.ieee.org/abstract/document/8803416">Paper</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Image Generation</span>
                                <span class="publication-tag">Facial Editing</span>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/zml2p.png" alt="Zero-Shot Facial Expression Recognition">
                                </div>
                                <h3 class="publication-title">Zero-Shot Facial Expression Recognition with Multi-Label Label Propagation</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Jiabei Zeng, Shiguang Shan, Xilin Chen</p>
                                <p class="publication-venue">ACCV 2018 Oral</p>
                            </div>
                            <div class="research-item-separator"></div> <!-- Added separator -->
                            <div class="research-item-right">
                                <p class="publication-description">Proposed a new Transductive Label Propagation method and the first Open-Set Facial Expression Recognition dataset.</p>
                            </div>
                        </div>
                        <div class="research-item-bottom">
                            <div class="publication-links">
                                <a href="https://link.springer.com/chapter/10.1007/978-3-030-20893-6_2">Paper</a>
                                <a href="https://github.com/ZijiaLewisLu/ACCV18-ZML2P">Code & Dataset</a>
                            </div>
                            <div class="publication-tags">
                                <span class="publication-tag">Affection Recognition</span>
                                <span class="publication-tag">Zero-Shot Learning</span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- News Section -->
            <div class="section" id="honors">
                <h2 class="section-title">Honors &amp; Awards</h2>
                <ul class="publications-list">
                    <li class="publication-item">
                        <p class="publication-venue">2019</p>
                        <p class="publication-description">NYU University Honors Scholar (for top-ranking graduates)</p>
                    </li>
                    <li class="publication-item">
                        <p class="publication-venue">2019</p>
                        <p class="publication-description">Undergraduate Scholarship of University of Chinese Academy of Sciences</p>
                    </li>
                    <li class="publication-item">
                        <p class="publication-venue">2017</p>
                        <p class="publication-description">Meritorious Winner of 2017 Interdisciplinary Contest in Modeling</p>
                    </li>
                    <li class="publication-item">
                        <p class="publication-venue">2016</p>
                        <p class="publication-description">NYU Shanghai Deans' Undergrad Research Fund</p>
                    </li>
                </ul>
            </div>
        </div>
    </div>
    
    <!-- Footer -->
    <footer>
        <p>¬© 2025 Zijia Lu. Last updated: April 2025</p>
    </footer>
</body>
</html>